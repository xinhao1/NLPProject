{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbBsspXcs0c1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Crawl tweets by API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "AEUqZNLms0c5",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "28c20937-bb4c-4794-8423-ec203402b847",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install tqdm\n",
    "# ! pip install tweepy\n",
    "# ! pip install torch\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6Cxu070s0c6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Crawl tweets by API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYSfaoCns0c7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you do not have data, uncomment main() to crawl tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "LDV-GKjNs0c7",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Tweet-Lookup/get_tweets_with_bearer_token.py\n",
    "# import requests\n",
    "# import json\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # To set your bearer token:\n",
    "# bearer_token = \"AAAAAAAAAAAAAAAAAAAAAGdZbgEAAAAAlXMiIg%2F96Ygnv%2FmvFDMsWb6LuSw%3DPTSIRz5g0G9RaB9pxp8QhdTtHxXnhEZsjLkpNyqQBR8EfRy8WS\"\n",
    "\n",
    "\n",
    "# def create_url(ids):\n",
    "#     tweet_fields = \"tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\"\n",
    "#     # Tweet fields are adjustable.\n",
    "#     # Options include:\n",
    "#     # attachments, author_id, context_annotations,\n",
    "#     # conversation_id, created_at, entities, geo, id,\n",
    "#     # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "#     # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "#     # source, text, and withheld\n",
    "#     ids = \"ids=\" + ids\n",
    "#     # print(ids)\n",
    "#     # You can adjust ids to include a single Tweets.\n",
    "#     # Or you can add to up to 100 comma-separated IDs\n",
    "#     url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids, tweet_fields)\n",
    "#     return url\n",
    "\n",
    "\n",
    "# def bearer_oauth(r):\n",
    "#     \"\"\"\n",
    "#     Method required by bearer token authentication.\n",
    "#     \"\"\"\n",
    "#     r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "#     r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "#     return r\n",
    "\n",
    "\n",
    "# def connect_to_endpoint(url):\n",
    "#     response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "#     # print(response.status_code)\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(\n",
    "#             \"Request returned an error: {} {}\".format(\n",
    "#                 response.status_code, response.text\n",
    "#             )\n",
    "#         )\n",
    "#     return response.json()\n",
    "\n",
    "\n",
    "# def crawl_and_save(f_in, f_out):\n",
    "#     train_id_list = []\n",
    "#     for l in f_in.readlines():\n",
    "#         train_id_list.extend(l.strip().split(\",\"))\n",
    "#     start_id = 0\n",
    "#     end_id = start_id + 100\n",
    "#     train_id_len = len(train_id_list)\n",
    "#     # max 100 tweet\n",
    "#     split_crawl = []\n",
    "#     while start_id < train_id_len:\n",
    "#         split_crawl.append(\",\".join(train_id_list[start_id:end_id]))\n",
    "#         start_id = end_id\n",
    "#         end_id = start_id + 100\n",
    "\n",
    "#     crawl_count = 0\n",
    "#     for ids in tqdm(split_crawl):\n",
    "#         url = create_url(ids)\n",
    "#         json_response = connect_to_endpoint(url)\n",
    "#         for x in json_response[\"data\"]:\n",
    "#             json.dump(x, open(f_out + str(x[\"id\"]) + \".json\", \"w\"))\n",
    "#         crawl_count += 1\n",
    "#         if crawl_count % 290 == 0:\n",
    "#             time.sleep(790)\n",
    "\n",
    "# # un-comment to crawl tweets\n",
    "# def main():\n",
    "#     print(\"crawl the train tweets\")\n",
    "#     # crawl_and_save(open(\"data/train.data.txt\", \"r\"), \"data/train_tweet/\")\n",
    "#     # print(\"crawl the dev tweets\")\n",
    "#     # crawl_and_save(open(\"data/dev.data.txt\", \"r\"), \"data/dev_tweet/\")\n",
    "#     # print(\"crawl the analysis tweets\")\n",
    "#     # crawl_and_save(open(\"data/covid.data.txt\", \"r\"), \"data/analysis_tweet/\")\n",
    "#     print(\"Finished!\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-yJPRUUs0c9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset read-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycZ5xA5Ls0c9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read-in tweets and labels, then sort one tweet with retweets by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "BGG_oJSNs0c-",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "train_ids = open(\"data/train.data.txt\", \"r\")\n",
    "train_labels = open(\"data/train.label.txt\", \"r\")\n",
    "dev_ids = open(\"data/dev.data.txt\", \"r\")\n",
    "dev_labels = open(\"data/dev.label.txt\", \"r\")\n",
    "\n",
    "def read_ids_labels(train_ids, train_labels, path = \"data/train_tweet/\"):\n",
    "    train_data = []\n",
    "    train_y = []\n",
    "    for train_ids_str, label in zip(train_ids.readlines(), train_labels.readlines()):\n",
    "        train_ids_list = train_ids_str.strip().split(\",\")\n",
    "        temp_json_list = []\n",
    "        if os.path.exists(path + train_ids_list[0] + \".json\"):\n",
    "            for train_id in train_ids_list:\n",
    "                train_path = path + train_id + \".json\"\n",
    "                if os.path.exists(train_path):\n",
    "                    tweet_json = json.load(open(train_path, \"r\"))\n",
    "                    if tweet_json not in temp_json_list:\n",
    "                        temp_json_list.append(tweet_json)\n",
    "                    # while tweet json has reference tweets, keep adding them to the list\n",
    "                    while \"referenced_tweets\" in tweet_json:\n",
    "                        referenced_tweets_id = tweet_json[\"referenced_tweets\"][0][\"id\"]\n",
    "                        if os.path.exists(path + referenced_tweets_id + \".json\"):\n",
    "                            tweet_json = json.load(open(path + referenced_tweets_id + \".json\", \"r\"))\n",
    "                            if tweet_json not in temp_json_list:\n",
    "                                temp_json_list.append(tweet_json)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            # sort the list by time\n",
    "            temp_json_list = sorted(temp_json_list, key=lambda x: x[\"created_at\"])\n",
    "            train_data.append(temp_json_list)\n",
    "            if label.strip() == \"rumour\":\n",
    "                train_y.append(1)\n",
    "            else:\n",
    "                train_y.append(0)\n",
    "    return train_data, train_y\n",
    "\n",
    "train_set, train_label = read_ids_labels(train_ids, train_labels)\n",
    "dev_set, dev_label = read_ids_labels(dev_ids, dev_labels, path=\"data/dev_tweet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uKBgt10rs0c_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_ids = open(\"data/test.data.txt\", \"r\")\n",
    "test_set = []\n",
    "for test_ids_str in test_ids.readlines():\n",
    "    test_ids_list = test_ids_str.strip().split(\",\")\n",
    "    temp_json_list = []\n",
    "    for test_id in test_ids_list:\n",
    "        test_path = \"data/tweet-objects/\" + test_id + \".json\"\n",
    "        if os.path.exists(test_path):\n",
    "            tweet_json = json.load(open(test_path, \"r\"))\n",
    "            if tweet_json not in temp_json_list:\n",
    "                temp_json_list.append(tweet_json)\n",
    "            # while tweet json has in_reply_to_status_id, keep adding them to the list\n",
    "            while tweet_json[\"in_reply_to_status_id\"] != None:\n",
    "                in_reply_to_status_id = str(tweet_json[\"in_reply_to_status_id\"])\n",
    "                if os.path.exists(\"data/tweet-objects/\" + in_reply_to_status_id + \".json\"):\n",
    "                    tweet_json = json.load(open(\"data/tweet-objects/\" + in_reply_to_status_id + \".json\", \"r\"))\n",
    "                    if tweet_json not in temp_json_list:\n",
    "                        temp_json_list.append(tweet_json)\n",
    "                else:\n",
    "                    break\n",
    "    temp_json_list = sorted(temp_json_list, key=lambda x: x[\"created_at\"])\n",
    "    test_set.append(temp_json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ7u4xq-s0c_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "JxSe7QTos0dA",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-mar2022 were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-mar2022 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load pretrained bert base model\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-mar2022')\n",
    "\n",
    "#load BERT's WordPiece tokenisation model\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-mar2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMANxUx9s0dA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Combine a tweet and its retweets into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "_Bsp-lL6s0dA",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def combine_tweet_retweet(train_set):\n",
    "    all_tweets = []\n",
    "    all_tokens = []\n",
    "    T = 512\n",
    "    all_padded_tokens = []\n",
    "\n",
    "    for tweets in train_set:\n",
    "        tweets_list = []\n",
    "        for tweet in tweets:\n",
    "            text = tweet[\"text\"]\n",
    "            text_list = []\n",
    "            # replace @user and http\n",
    "            for word in text.split(\" \"):\n",
    "                if len(word) > 1 and word[0] == \"@\":\n",
    "                    text_list.append(\"@\")\n",
    "                elif len(word) > 4 and word[0:4] == \"http\":\n",
    "                    text_list.append(\"HTTP\")\n",
    "                else:\n",
    "                    text_list.append(word)\n",
    "            new_text = \" \".join(text_list)\n",
    "            tweets_list.append(new_text)\n",
    "        new_text = \"[CLS]\" + \"[SEP]\".join(tweets_list) + \"[SEP]\"\n",
    "        all_tweets.append(new_text)\n",
    "\n",
    "    return all_tweets\n",
    "\n",
    "train_text = combine_tweet_retweet(train_set)\n",
    "dev_text = combine_tweet_retweet(dev_set)\n",
    "test_text = combine_tweet_retweet(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "id": "G4uKOdhNs0dB",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# make a dataframe with the tweet text and labels\n",
    "train_df = pd.DataFrame({\"text\": train_text, \"label\": train_label})\n",
    "dev_df = pd.DataFrame({\"text\": dev_text, \"label\": dev_label})\n",
    "test_df = pd.DataFrame({\"text\": test_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "id": "xipzXw2as0dB",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, maxlen):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = df\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-mar2022')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'text']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "id": "iN4jNwscs0dC",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing training and development data.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and development set\n",
    "#maxlen sets the maximum length a sentence can have\n",
    "#any sentence longer than this length is truncated to the maxlen size\n",
    "train_set = SSTDataset(train_df, maxlen = 512)\n",
    "dev_set = SSTDataset(dev_df, maxlen = 512)\n",
    "#Creating intsances of training and development dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 16, num_workers = 2)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 16, num_workers = 2)\n",
    "\n",
    "print(\"Done preprocessing training and development data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "0nVCGcgxs0dC",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class RumourClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumourClassifier, self).__init__()\n",
    "        #Instantiating BERT model object\n",
    "        self.bert_layer = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-mar2022')\n",
    "\n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "R3OTnDgds0dD",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the rumour classifier, initialised with pretrained BERT-BASE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-mar2022 were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-mar2022 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the rumour classifier.\n"
     ]
    }
   ],
   "source": [
    "torch.no_grad()\n",
    "gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the rumour classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = RumourClassifier()\n",
    "net.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the rumour classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "id": "XMSaXsW2s0dD",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "id": "QfNdxSEjs0dD",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_ep = None\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "\n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()\n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "\n",
    "        dev_acc, dev_loss, dev_f1 = evaluate(net, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}; Development f1-score: {}\".format(ep, dev_acc, dev_loss, dev_f1))\n",
    "        if dev_f1 > best_f1:\n",
    "            print(\"Best development f1-score improved from {} to {}, saving model...\".format(best_f1, dev_f1))\n",
    "            best_f1 = dev_f1\n",
    "            torch.save(net.state_dict(), 'sstcls.dat')\n",
    "\n",
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def get_f1_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(labels.cpu().numpy(), soft_probs.squeeze().cpu().numpy(), average = 'macro')\n",
    "    return f1\n",
    "\n",
    "def evaluate(net, criterion, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    mean_f1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "            mean_f1 += get_f1_from_logits(logits, labels)\n",
    "\n",
    "    return mean_acc / count, mean_loss / count, mean_f1 / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "id": "BQEOkoADs0dE",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "\n",
    "#fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PFzd2FFs0dE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### If having best model, load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RjQf8sm5s0dE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the best model.\n"
     ]
    }
   ],
   "source": [
    "torch.no_grad()\n",
    "\n",
    "net.load_state_dict(torch.load('sstcls.dat'))\n",
    "#net.eval()\n",
    "print(\"Load the best model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def predict(test_text, maxlen = 512):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-mar2022')\n",
    "\n",
    "    sentences_tensor = []\n",
    "    attn_mask_ts = []\n",
    "\n",
    "    for sentence in tqdm(test_text):\n",
    "\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        if len(tokens) < maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))] \n",
    "        else:\n",
    "            tokens = tokens[:maxlen-1] + ['[SEP]'] \n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids_tensor = torch.tensor(token_ids).unsqueeze(0)\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).unsqueeze(0)\n",
    "\n",
    "        sentences_tensor.append(token_ids_tensor)\n",
    "        attn_mask_ts.append(attn_mask_tensor)\n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(attn_mask_ts))):\n",
    "\n",
    "        prediction = net(sentences_tensor[i].cuda(gpu),attn_mask_ts[i].cuda(gpu))\n",
    "        probs = torch.sigmoid(prediction.unsqueeze(-1))\n",
    "        soft_probs = (probs > 0.5).long()\n",
    "        predictions.append(soft_probs.squeeze().tolist())\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/558 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 558/558 [00:01<00:00, 451.76it/s]\n",
      "100%|██████████| 558/558 [00:09<00:00, 59.69it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6yCYjwCYs0dF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# write sbmission file\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i, label in enumerate(predictions):\n",
    "        f.write(\"{},{}\\n\".format(i, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbAXkXTos0dF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Prepare data for task2 analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read-in done.\n",
      "combination done.\n"
     ]
    }
   ],
   "source": [
    "analysis_ids = open(\"data/covid.data.txt\", \"r\")\n",
    "analysis_set = []\n",
    "def read_ids(train_ids, path = \"data/analysis_tweet/\"):\n",
    "    train_data = []\n",
    "    train_y = []\n",
    "    for train_ids_str in train_ids.readlines():\n",
    "        train_ids_list = train_ids_str.strip().split(\",\")\n",
    "        temp_json_list = []\n",
    "        if os.path.exists(path + train_ids_list[0] + \".json\"):\n",
    "            for train_id in train_ids_list:\n",
    "                train_path = path + train_id + \".json\"\n",
    "                if os.path.exists(train_path):\n",
    "                    tweet_json = json.load(open(train_path, \"r\"))\n",
    "                    if tweet_json not in temp_json_list:\n",
    "                        temp_json_list.append(tweet_json)\n",
    "                    # while tweet json has reference tweets, keep adding them to the list\n",
    "                    while \"referenced_tweets\" in tweet_json:\n",
    "                        referenced_tweets_id = tweet_json[\"referenced_tweets\"][0][\"id\"]\n",
    "                        if os.path.exists(path + referenced_tweets_id + \".json\"):\n",
    "                            tweet_json = json.load(open(path + referenced_tweets_id + \".json\", \"r\"))\n",
    "                            if tweet_json not in temp_json_list:\n",
    "                                temp_json_list.append(tweet_json)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            # sort the list by time\n",
    "            temp_json_list = sorted(temp_json_list, key=lambda x: x[\"created_at\"])\n",
    "            train_data.append(temp_json_list)\n",
    "    return train_data\n",
    "\n",
    "analysis_set = read_ids(analysis_ids)\n",
    "print(\"Read-in done.\")\n",
    "analysis_text = combine_tweet_retweet(analysis_set)\n",
    "print(\"combination done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15956 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 15956/15956 [00:41<00:00, 384.39it/s]\n",
      "100%|██████████| 15956/15956 [04:12<00:00, 63.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(analysis_text)\n",
    "print(\"Prediction done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into two files: rumour.jsonl and nonrumour.jsonl\n",
    "rumour_file = \"rumour.jsonl\"\n",
    "nonrumour_file = \"nonrumour.jsonl\"\n",
    "\n",
    "rumour_writer = open(rumour_file, \"w\")\n",
    "nonrumour_writer = open(nonrumour_file, \"w\")\n",
    "for x, y in zip(predictions, analysis_set):\n",
    "    # get the source tweet which is the first one\n",
    "    y = y[0]\n",
    "    if x == 0:\n",
    "        nonrumour_writer.write(json.dumps(y) + \"\\n\")\n",
    "    else:\n",
    "        rumour_writer.write(json.dumps(y) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "task1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
