{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Crawl tweets by API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install tqdm\n",
    "# ! pip install tweepy\n",
    "# ! pip install torch\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Crawl tweets by API"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you do not have data, uncomment main() to crawl tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl the train tweets\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Tweet-Lookup/get_tweets_with_bearer_token.py\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To set your bearer token:\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAGdZbgEAAAAAlXMiIg%2F96Ygnv%2FmvFDMsWb6LuSw%3DPTSIRz5g0G9RaB9pxp8QhdTtHxXnhEZsjLkpNyqQBR8EfRy8WS\"\n",
    "\n",
    "\n",
    "def create_url(ids):\n",
    "    tweet_fields = \"tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\"\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    ids = \"ids=\" + ids\n",
    "    # print(ids)\n",
    "    # You can adjust ids to include a single Tweets.\n",
    "    # Or you can add to up to 100 comma-separated IDs\n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids, tweet_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "    # print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def crawl_and_save(f_in, f_out):\n",
    "    train_id_list = []\n",
    "    for l in f_in.readlines():\n",
    "        train_id_list.extend(l.strip().split(\",\"))\n",
    "    start_id = 0\n",
    "    end_id = start_id + 100\n",
    "    train_id_len = len(train_id_list)\n",
    "    # max 100 tweet\n",
    "    split_crawl = []\n",
    "    while start_id < train_id_len:\n",
    "        split_crawl.append(\",\".join(train_id_list[start_id:end_id]))\n",
    "        start_id = end_id\n",
    "        end_id = start_id + 100\n",
    "\n",
    "    crawl_count = 0\n",
    "    for ids in tqdm(split_crawl):\n",
    "        url = create_url(ids)\n",
    "        json_response = connect_to_endpoint(url)\n",
    "        for x in json_response[\"data\"]:\n",
    "            json.dump(x, open(f_out + str(x[\"id\"]) + \".json\", \"w\"))\n",
    "        crawl_count += 1\n",
    "        if crawl_count % 290 == 0:\n",
    "            time.sleep(790)\n",
    "\n",
    "# un-comment to crawl tweets\n",
    "def main():\n",
    "    print(\"crawl the train tweets\")\n",
    "    #crawl_and_save(open(\"data/train.data.txt\", \"r\"), \"data/train_tweet/\")\n",
    "    # print(\"crawl the dev tweets\")\n",
    "    # crawl_and_save(open(\"data/dev.data.txt\", \"r\"), \"data/dev_tweet/\")\n",
    "    # print(\"crawl the analysis tweets\")\n",
    "    # crawl_and_save(open(\"data/covid.data.txt\", \"r\"), \"data/analysis_tweet/\")\n",
    "    print(\"Finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset read-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read-in tweets and labels, then sort one tweet with retweets by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "train_ids = open(\"data/train.data.txt\", \"r\")\n",
    "train_labels = open(\"data/train.label.txt\", \"r\")\n",
    "dev_ids = open(\"data/dev.data.txt\", \"r\")\n",
    "dev_labels = open(\"data/dev.label.txt\", \"r\")\n",
    "\n",
    "def read_ids_labels(ids, labels):\n",
    "    train_set = []\n",
    "    train_label = []\n",
    "    for train_ids_str, label in zip(ids.readlines(), labels.readlines()):\n",
    "        train_ids_list = train_ids_str.strip().split(\",\")\n",
    "        temp_json_list = []\n",
    "        if os.path.exists(\"data/train_tweet/\" + train_ids_list[0] + \".json\"):\n",
    "            for train_id in train_ids_list:\n",
    "                train_path = \"data/train_tweet/\" + train_id + \".json\"\n",
    "                if os.path.exists(train_path):\n",
    "                    temp_json_list.append(json.load(open(train_path, \"r\")))\n",
    "        # sort according to time\n",
    "        temp_json_list = sorted(temp_json_list, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%Y-%m-%dT%H:%M:%S.%fZ')))\n",
    "        train_set.append(temp_json_list)\n",
    "        if label.strip() == \"rumour\":\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "\n",
    "    return train_set, train_label\n",
    "\n",
    "train_set, train_label = read_ids_labels(train_ids, train_labels)\n",
    "dev_set, dev_label = read_ids_labels(dev_ids, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_ids = open(\"data/test.data.txt\", \"r\")\n",
    "test_set = []\n",
    "for test_ids_str in test_ids.readlines():\n",
    "    test_ids_list = test_ids_str.strip().split(\",\")\n",
    "    temp_json_list = []\n",
    "    for test_id in test_ids_list:\n",
    "        test_path = \"data/tweet-objects/\" + test_id + \".json\"\n",
    "        if os.path.exists(test_path):\n",
    "            tweet_json = json.load(open(test_path, \"r\"))\n",
    "            if tweet_json not in temp_json_list:\n",
    "                temp_json_list.append(tweet_json)\n",
    "            # while tweet json has in_reply_to_status_id, keep adding them to the list\n",
    "            while tweet_json[\"in_reply_to_status_id\"] != None:\n",
    "                in_reply_to_status_id = str(tweet_json[\"in_reply_to_status_id\"])\n",
    "                if os.path.exists(\"data/tweet-objects/\" + in_reply_to_status_id + \".json\"):\n",
    "                    tweet_json = json.load(open(\"data/tweet-objects/\" + in_reply_to_status_id + \".json\", \"r\"))\n",
    "                    if tweet_json not in temp_json_list:\n",
    "                        temp_json_list.append(tweet_json)\n",
    "                else:\n",
    "                    break\n",
    "    temp_json_list = sorted(temp_json_list, key=lambda x: x[\"created_at\"])\n",
    "    test_set.append(temp_json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#load pretrained bert base model\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#load BERT's WordPiece tokenisation model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Combine a tweet and its retweets into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def combine_tweet_retweet(train_set):\n",
    "    all_tweets = []\n",
    "    all_tokens = []\n",
    "    T = 512\n",
    "    all_padded_tokens = []\n",
    "\n",
    "    for tweets in train_set:\n",
    "        tweets_list = []\n",
    "        for tweet in tweets:\n",
    "            text = tweet[\"text\"]\n",
    "            text_list = []\n",
    "            # replace @user and http\n",
    "            for word in text.split(\" \"):\n",
    "                if len(word) > 1 and word[0] == \"@\":\n",
    "                    text_list.append(\"@\")\n",
    "                elif len(word) > 4 and word[0:4] == \"http\":\n",
    "                    text_list.append(\"HTTP\")\n",
    "                else:\n",
    "                    text_list.append(word)\n",
    "            new_text = \" \".join(text_list)\n",
    "            tweets_list.append(new_text)\n",
    "        new_text = \"[CLS]\" + \"[SEP]\".join(tweets_list) + \"[SEP]\"\n",
    "        all_tweets.append(new_text)\n",
    "        tokens = tokenizer.tokenize(new_text)\n",
    "        all_tokens.append(tokens)\n",
    "        # pad tokens\n",
    "        padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))]\n",
    "        all_padded_tokens.append(padded_tokens)\n",
    "        # attention mask\n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "        # seg id\n",
    "        seg_ids = [0 for _ in range(len(padded_tokens))]\n",
    "        # token id\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "    return all_tweets, all_tokens, padded_tokens, attn_mask, seg_ids, token_ids\n",
    "\n",
    "train_text, train_tokens, train_padded_tokens, train_attn_mask, train_seg_ids, train_token_ids = combine_tweet_retweet(train_set)\n",
    "dev_text, dev_tokens, dev_padded_tokens, dev_attn_mask, dev_seg_ids, dev_token_ids = combine_tweet_retweet(dev_set)\n",
    "test_text, test_tokens, test_padded_tokens, test_attn_mask, test_seg_ids, test_token_ids = combine_tweet_retweet(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "tensor([ 0.0340, -0.0099,  0.0684,  0.1292, -0.4194, -0.6827,  0.4952,  0.3319,\n",
      "         0.0633, -0.5607], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#Converting all the input vectors to torch tensors\n",
    "token_ids_t = torch.tensor(train_token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "attn_mask_t = torch.tensor(train_attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "seg_ids_t   = torch.tensor(train_seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "\n",
    "#Feed them to bert and get the contextualised embeddings\n",
    "outputs = bert_model(token_ids_t, attention_mask = attn_mask_t,\\\n",
    "                                  token_type_ids = seg_ids_t, return_dict=True)\n",
    "hidden_reps = outputs.last_hidden_state\n",
    "print(hidden_reps.shape)\n",
    "print(hidden_reps[0, 0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# make a dataframe with the tweet text and labels\n",
    "train_df = pd.DataFrame({\"text\": train_text, \"label\": train_label})\n",
    "dev_df = pd.DataFrame({\"text\": dev_text, \"label\": dev_label})\n",
    "test_df = pd.DataFrame({\"text\": test_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, maxlen):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = df\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'text']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing training and development data.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and development set\n",
    "#maxlen sets the maximum length a sentence can have\n",
    "#any sentence longer than this length is truncated to the maxlen size\n",
    "train_set = SSTDataset(train_df, maxlen = 512)\n",
    "dev_set = SSTDataset(dev_df, maxlen = 512)\n",
    "#Creating intsances of training and development dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 16, num_workers = 2)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 16, num_workers = 2)\n",
    "\n",
    "print(\"Done preprocessing training and development data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        #Instantiating BERT model object\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the sentiment classifier.\n"
     ]
    }
   ],
   "source": [
    "gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = SentimentClassifier()\n",
    "net.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the sentiment classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
    "\n",
    "    best_acc = 0\n",
    "    best_ep = None\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "\n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()\n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "\n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))\n",
    "            best_ep = ep\n",
    "    return best_ep\n",
    "\n",
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 5\n",
    "\n",
    "#fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### If having best model, load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the best model.\n"
     ]
    }
   ],
   "source": [
    "torch.no_grad()\n",
    "gpu = 0 #gpu ID\n",
    "\n",
    "net = SentimentClassifier()\n",
    "net.cuda(gpu) #Enable gpu support for the model\n",
    "\n",
    "net.load_state_dict(torch.load('sstcls_3.dat'))\n",
    "net.eval()\n",
    "print(\"Load the best model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_bert(test_text, maxlen = 512):\n",
    "    from transformers import BertTokenizer\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    sentences_tensor = []\n",
    "    attn_mask_ts = []\n",
    "\n",
    "    for sentence in test_text:\n",
    "\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        if len(tokens) < maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))] \n",
    "        else:\n",
    "            tokens = tokens[:maxlen-1] + ['[SEP]'] \n",
    "        attn_mask_1 = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids_tensor = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, maxlen]\n",
    "        attn_mask_tensor = torch.tensor(attn_mask_1).unsqueeze(0) #Shape : [1, maxlen]\n",
    "\n",
    "        sentences_tensor.append(token_ids_tensor)\n",
    "        attn_mask_ts.append(attn_mask_tensor)\n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(attn_mask_ts)):\n",
    "\n",
    "        prediction = net(sentences_tensor[i].cuda(gpu),attn_mask_ts[i].cuda(gpu))\n",
    "        probs = torch.sigmoid(prediction.unsqueeze(-1))\n",
    "        soft_probs = (probs > 0.5).long()\n",
    "        predictions.append(soft_probs.squeeze().tolist())\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "predictions = prediction_bert(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# write sbmission file\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i, label in enumerate(predictions):\n",
    "        f.write(\"{},{}\\n\".format(i, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}